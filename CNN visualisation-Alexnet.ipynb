{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3293087d-c331-4f03-87d7-cbb540805f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db26331-bd57-4162-a504-9d4676935d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read annotation data from the .cat files\n",
    "def read_annotation_file(annotation_file_path):\n",
    "    with open(annotation_file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        values = line.split()\n",
    "        num_points = int(values[0])\n",
    "        annotation_data = [int(value) for value in values[1:]]\n",
    "        return num_points, annotation_data\n",
    "\n",
    "# Path to the directory containing the images and .cat files\n",
    "folder_path = r'C:\\Users\\haris\\AI\\ML\\Projects\\Code\\Learning\\CAT_00'\n",
    "\n",
    "# Lists to store paths of .jpg files and .cat files\n",
    "image_files_path = []\n",
    "annotation_files_path = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', 'png')):\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_files_path.append(image_path)\n",
    "\n",
    "        if file.lower().endswith('.cat'):\n",
    "            annotation_path = os.path.join(root, file)\n",
    "            annotation_files_path.append(annotation_path)\n",
    "\n",
    "# Lists to store resized image data and annotation data\n",
    "resized_images = []\n",
    "resized_annotations = []\n",
    "\n",
    "image_size_alexnet = (227, 227)\n",
    "\n",
    "# Read annotation data for each image and apply normalization\n",
    "for annotation_file_path in annotation_files_path:\n",
    "    num_points, annotation_data = read_annotation_file(annotation_file_path)\n",
    "\n",
    "    image = cv2.imread(annotation_file_path[:-4], cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image, image_size_Lenet)\n",
    "\n",
    "    # Calculate the scaling factor for annotation coordinates\n",
    "    scale_y = resized_image.shape[0] / image.shape[0]\n",
    "    scale_x = resized_image.shape[1] / image.shape[1]\n",
    "\n",
    "    # Apply scaling to the annotation data for both x and y coordinates together\n",
    "    resized_annotation_data = [int(value * scale_x) if i % 2 == 0 else int(value * scale_y) for i, value in enumerate(annotation_data)]\n",
    "\n",
    "    resized_images.append(resized_image)\n",
    "    resized_annotations.append(resized_annotation_data)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "resized_images = np.array(resized_images)\n",
    "resized_annotations = np.array(resized_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5809d6c4-f0d7-4689-ae77-a81e53706851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(resized_images, resized_annotations, test_size=0.1, random_state=42)\n",
    "\n",
    "# Function to load and preprocess the images and annotations in batches\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc6698b-91f9-42ed-bb8e-459077dbf83e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AlexNet' from 'tensorflow.keras.applications' (C:\\Users\\haris\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\api\\_v2\\keras\\applications\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlexNet\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AlexNet' from 'tensorflow.keras.applications' (C:\\Users\\haris\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\api\\_v2\\keras\\applications\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import AlexNet\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Load pre-trained AlexNet model\n",
    "pretrained_alexnet = AlexNet(weights='imagenet', include_top=False, input_shape=(227, 227, 3))\n",
    "\n",
    "# Remove top layers\n",
    "x = pretrained_alexnet.output\n",
    "\n",
    "# Add new layers for your regression task\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "predictions = Dense(18, activation='linear')(x)\n",
    "\n",
    "# Create a new model with modified layers\n",
    "model = Model(inputs=pretrained_alexnet.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "initial_learning_rate = 0.0001\n",
    "optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with data generator\n",
    "batch_size = 4\n",
    "train_gen = DataGenerator(x_train, y_train, batch_size)\n",
    "test_gen = DataGenerator(x_test, y_test, batch_size)\n",
    "\n",
    "# Train the model with data generator\n",
    "history = AlexNet_model.fit(\n",
    "    train_gen,\n",
    "    epochs=20,\n",
    "    validation_data=test_gen,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9a9e2f-59cb-4f14-8a0c-e30cd316b51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Plot the training history\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m plot_training_history(\u001b[43mhistory\u001b[49m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Print the summary of the AlexNet model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m AlexNet_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to plot the training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error (MAE)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Print the summary of the AlexNet model\n",
    "AlexNet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87ea26-89be-4905-ab4d-c47af347f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_path = r'C:\\Users\\haris\\AI\\ML\\Projects\\Code\\Learning\\CAT_00\\00000324_011.jpg'\n",
    "image = cv2.imread(new_image_path, cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "resized_image = cv2.resize(image, image_size_Lenet)\n",
    "input_image = np.expand_dims(resized_image, axis=0)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = Lenet_model.predict(input_image)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a78bc-a0ff-41c8-ac52-22cb4ea5b101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Function to predict facial landmarks on new images\n",
    "def predict_landmarks(image_input):\n",
    "    # Convert Gradio image object to numpy array\n",
    "    image = image_input.astype('uint8')\n",
    "\n",
    "    # Define the image size for resizing\n",
    "    image_size = (32, 32)\n",
    "\n",
    "    # Convert to RGB before resizing\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    resized_image = cv2.resize(image_rgb, image_size)\n",
    "    input_image = np.expand_dims(resized_image, axis=0)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = Lenet_model.predict(input_image)\n",
    "\n",
    "    # Rescale the predictions to the original image size\n",
    "    scale_y = image.shape[0] / image_size[0]\n",
    "    scale_x = image.shape[1] / image_size[1]\n",
    "    resized_predictions = [int(value * scale_x) if i % 2 == 0 else int(\n",
    "        value * scale_y) for i, value in enumerate(predictions[0])]\n",
    "\n",
    "    # Calculate the radius of the circles based on image dimensions\n",
    "    image_height, image_width, _ = image.shape\n",
    "    max_dim = max(image_height, image_width)\n",
    "    radius_scale = max_dim / 1500  # Adjust this scale factor as needed\n",
    "    \n",
    "    # Draw circles (dots) on the original image at the predicted landmark locations\n",
    "    for i in range(0, len(resized_predictions), 2):\n",
    "        x, y = resized_predictions[i], resized_predictions[i + 1]\n",
    "        color = (255, 0, 0)\n",
    "        radius = int(8 * radius_scale)  # Adjust the base radius value as needed\n",
    "        thickness = -1\n",
    "        cv2.circle(image, (x, y), radius, color, thickness)\n",
    "        \n",
    "    return image\n",
    "\n",
    "# Create the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    predict_landmarks, \n",
    "    inputs = \"image\",\n",
    "    outputs = \"image\",\n",
    "    title = \"Cat Facial Landmark Predictor\",\n",
    "    description=\"Upload an image of a cat's face to predict its facial landmarks.\",\n",
    "    cache_examples=True,\n",
    "    live=True,\n",
    "    theme=\"default\",\n",
    "    allow_flagging=\"manual\",\n",
    "    flagging_options=[\"Flag as incorrect\", \"Flag as inaccurate\"],\n",
    "    analytics_enabled=True,\n",
    "    batch=False,\n",
    "    max_batch_size=4,\n",
    "    allow_duplication=False\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cefd39-fb88-47b2-9702-03db46aa772d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
